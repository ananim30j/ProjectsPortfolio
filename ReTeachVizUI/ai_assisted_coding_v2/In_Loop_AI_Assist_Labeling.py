# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/In_Loop_AI_Assist_Labeling.ipynb.

# %% auto 0
__all__ = ['BertLabeler']

# %% ../nbs/In_Loop_AI_Assist_Labeling.ipynb 5
import sys
sys.path.append('../')
from transformers import Trainer, TrainingArguments, DataCollatorWithPadding
from datasets import Dataset
from . import AI_Assist_Labeling

# Define the BertLabeler class, inheriting from AI_Assist_Labeling.BertBase
class BertLabeler(AI_Assist_Labeling.BertBase):

    # Retrieve the current DataFrame
    def get_df(self):
        return self.df

    # Set the DataFrame with new data
    def set_df(self, dataframe):
        self.df = dataframe

    # Method to get a specific slice of the dataframe based on start and end row indices
    def get_slice(self, start, end):
        return self.df[start:end]

    # Method to label a single sentence using the trained classifier
    def label_sentence(self, sentence):
        descriptive_labels = list(self.label_map.keys())
        result = self.classifier(sentence, descriptive_labels)
        # Convert descriptive label to acronym and return it along with the confidence score
        return self.label_map[result['labels'][0]], result['scores'][0]
    
    # Method to label a list of sentences
    def label_list_sentences(self, sentences):
        # Iterate over each sentence, labeling it and collecting the results
        return [self.label_sentence(sentence) for sentence in sentences]
    
    # Method to calculate the accuracy of predictions in a batch and adjust the batch size accordingly
    def accuracy_batch_calculation(self, predicted, actual, batch_size):
        if len(predicted) != len(actual):
            raise ValueError("Lists must be of the same length")

        # Count the number of correct predictions
        matches = sum(1 for x, y in zip(predicted, actual) if x == y)

        # Calculate the accuracy as a percentage
        accuracy = (matches / len(predicted)) * 100

        # Adjust batch size based on accuracy using a simple heuristic
        if accuracy > 50:
            batch_size += 5
        else:
            if batch_size > 10:
                batch_size -= 5

        return accuracy, batch_size

    # Method for training the model with new data
    def train_with_sentences(self, sentences, labels, model_save_path='../trained_model', save_model=False):
        # Convert text labels to numeric IDs
        label_to_id = {v: k for k, v in enumerate(set(labels))}
        labels = [label_to_id[label] for label in labels]

        # Tokenize the input sentences
        train_encodings = self.tokenizer(sentences, truncation=True, padding=True, max_length=512)

        # Create a dataset from the tokenized sentences and labels
        train_dataset = Dataset.from_dict({
            'input_ids': train_encodings['input_ids'],
            'attention_mask': train_encodings['attention_mask'],
            'labels': labels
        })

        # Define training arguments for the model
        training_args = TrainingArguments(
            output_dir=model_save_path,
            num_train_epochs=3,  # Number of training epochs
            per_device_train_batch_size=1,  # Small batch size for detailed updates
            warmup_steps=500,  # Number of warmup steps
            weight_decay=0.01,  # Weight decay for regularization
            logging_dir='./logs',  # Directory for logs
            save_steps=5,  # Frequency of model saving
            save_total_limit=3,  # Maximum number of saved models
            load_best_model_at_end=False  # Flag to control model loading behavior
        )

        # Initialize the Trainer with the model, training arguments, dataset, and collator
        trainer = Trainer(
            model=self.model,                  
            args=training_args,
            train_dataset=train_dataset,
            data_collator=DataCollatorWithPadding(tokenizer=self.tokenizer),
            tokenizer=self.tokenizer
        )

        # Start the training process
        trainer.train()

        # Save the model and tokenizer if required
        if save_model:
            self.model.save_pretrained(model_save_path)
            self.tokenizer.save_pretrained(model_save_path)

