{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Loop AI Assisted Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "This Jupyter notebook is a crucial component of the ReTeach project, focusing on the in-the-loop AI-assisted labeling task. It is designed to enhance the process of labeling educational transcripts by integrating machine learning models and human expertise. The notebook allows users to interactively label classroom session transcripts, thereby training the model to improve its labeling accuracy over time.\n",
    "\n",
    "### Features\n",
    "**Data Loading and Preprocessing**:\n",
    "\n",
    "This section deals with importing classroom session transcripts from CSV files, and slice datasets when needed by streamlit UI.\n",
    "\n",
    "**AI Labeling Initialization**:\n",
    "\n",
    "Loading BERT Encoder model for initial label predictions. The model predicts labels for the transcript text for each prediction.\n",
    "\n",
    "**Model Training and Adaptation**:\n",
    "\n",
    "The notebook includes functionality to retrain the model with the corrected labels, enhancing its prediction accuracy over time.\n",
    "The process of model adaptation is iteratively repeated, improving the model's performance with each batch of labeled data.\n",
    "\n",
    "**Batch Processing and Accuracy Tracking**:\n",
    "\n",
    "Users can process variable numbers of transcript lines in batches based on previous batch accuracy.\n",
    "The notebook tracks and displays the accuracy of the modelâ€™s predictions, adjusting the batch sizes based on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on model used (BERT)\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a groundbreaking model in the field of natural language processing (NLP) developed by Google. It represents a significant leap forward because of its deep understanding of language context and nuance. Unlike previous models that analyzed text in one direction, either left-to-right or right-to-left, BERT is bidirectional, meaning it considers the full context of a word by looking at the words that come before and after it. This feature enables BERT to capture a more comprehensive understanding of language structure and meaning.\n",
    "\n",
    "For the task of labeling educational transcripts, BERT is particularly suitable due to its exceptional ability to understand the context and nuances of human language. Educational transcripts often contain complex sentences, specialized terminology, and varied expressions that require a deep understanding of context to accurately interpret and label. BERT's proficiency in understanding context allows it to accurately classify sentences into categories such as praise, reprimand, or neutral remarks, which are typical in educational settings. Additionally, BERT's versatility and adaptability make it ideal for custom tasks like this, where it can be fine-tuned with specific data (like educational transcripts) to enhance its performance in a specialized domain. This ability to adapt to the nuances of educational dialogue makes BERT an excellent choice for the automated labeling of classroom transcripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "1. This notebook does NOT contain all components for this particular task, some of the project requirements (such as user interface) are done via streamlit in app.py file.\n",
    "\n",
    "2. Due to complexity of BERT model, and lack of extensive label training dataset, the training improvement might not be significant. This could be done once we have access to larger labeling dataset as well as extensive fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp In_Loop_AI_Assist_Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "from ai_assisted_coding_v2 import AI_Assist_Labeling\n",
    "\n",
    "# Define the BertLabeler class, inheriting from AI_Assist_Labeling.BertBase\n",
    "class BertLabeler(AI_Assist_Labeling.BertBase):\n",
    "\n",
    "    # Retrieve the current DataFrame\n",
    "    def get_df(self):\n",
    "        return self.df\n",
    "\n",
    "    # Set the DataFrame with new data\n",
    "    def set_df(self, dataframe):\n",
    "        self.df = dataframe\n",
    "\n",
    "    # Method to get a specific slice of the dataframe based on start and end row indices\n",
    "    def get_slice(self, start, end):\n",
    "        return self.df[start:end]\n",
    "\n",
    "    # Method to label a single sentence using the trained classifier\n",
    "    def label_sentence(self, sentence):\n",
    "        descriptive_labels = list(self.label_map.keys())\n",
    "        result = self.classifier(sentence, descriptive_labels)\n",
    "        # Convert descriptive label to acronym and return it along with the confidence score\n",
    "        return self.label_map[result['labels'][0]], result['scores'][0]\n",
    "    \n",
    "    # Method to label a list of sentences\n",
    "    def label_list_sentences(self, sentences):\n",
    "        # Iterate over each sentence, labeling it and collecting the results\n",
    "        return [self.label_sentence(sentence) for sentence in sentences]\n",
    "    \n",
    "    # Method to calculate the accuracy of predictions in a batch and adjust the batch size accordingly\n",
    "    def accuracy_batch_calculation(self, predicted, actual, batch_size):\n",
    "        if len(predicted) != len(actual):\n",
    "            raise ValueError(\"Lists must be of the same length\")\n",
    "\n",
    "        # Count the number of correct predictions\n",
    "        matches = sum(1 for x, y in zip(predicted, actual) if x == y)\n",
    "\n",
    "        # Calculate the accuracy as a percentage\n",
    "        accuracy = (matches / len(predicted)) * 100\n",
    "\n",
    "        # Adjust batch size based on accuracy using a simple heuristic\n",
    "        if accuracy > 50:\n",
    "            batch_size += 5\n",
    "        else:\n",
    "            if batch_size > 10:\n",
    "                batch_size -= 5\n",
    "\n",
    "        return accuracy, batch_size\n",
    "\n",
    "    # Method for training the model with new data\n",
    "    def train_with_sentences(self, sentences, labels, model_save_path='../trained_model', save_model=False):\n",
    "        # Convert text labels to numeric IDs\n",
    "        label_to_id = {v: k for k, v in enumerate(set(labels))}\n",
    "        labels = [label_to_id[label] for label in labels]\n",
    "\n",
    "        # Tokenize the input sentences\n",
    "        train_encodings = self.tokenizer(sentences, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "        # Create a dataset from the tokenized sentences and labels\n",
    "        train_dataset = Dataset.from_dict({\n",
    "            'input_ids': train_encodings['input_ids'],\n",
    "            'attention_mask': train_encodings['attention_mask'],\n",
    "            'labels': labels\n",
    "        })\n",
    "\n",
    "        # Define training arguments for the model\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=model_save_path,\n",
    "            num_train_epochs=3,  # Number of training epochs\n",
    "            per_device_train_batch_size=1,  # Small batch size for detailed updates\n",
    "            warmup_steps=500,  # Number of warmup steps\n",
    "            weight_decay=0.01,  # Weight decay for regularization\n",
    "            logging_dir='./logs',  # Directory for logs\n",
    "            save_steps=5,  # Frequency of model saving\n",
    "            save_total_limit=3,  # Maximum number of saved models\n",
    "            load_best_model_at_end=False  # Flag to control model loading behavior\n",
    "        )\n",
    "\n",
    "        # Initialize the Trainer with the model, training arguments, dataset, and collator\n",
    "        trainer = Trainer(\n",
    "            model=self.model,                  \n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            data_collator=DataCollatorWithPadding(tokenizer=self.tokenizer),\n",
    "            tokenizer=self.tokenizer\n",
    "        )\n",
    "\n",
    "        # Start the training process\n",
    "        trainer.train()\n",
    "\n",
    "        # Save the model and tokenizer if required\n",
    "        if save_model:\n",
    "            self.model.save_pretrained(model_save_path)\n",
    "            self.tokenizer.save_pretrained(model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertBase is being initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('NEU', 0.26569560170173645)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demo single sentence labeling\n",
    "bert = BertLabeler(model_name=\"bert-base-uncased\")\n",
    "bert.label_sentence(\"I don't know\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ../trained_model/checkpoint-10 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    }
   ],
   "source": [
    "# Training example, using a list of sentences and labels\n",
    "sentences = [\"Good morning class, today we are going to learn about nouns.\",\n",
    "             \"I'm going to give you a chance to answer a question.\",\n",
    "             \"You are all doing a great job.\",\n",
    "             \"You are all doing a terrible job.\"]\n",
    "\n",
    "labels = [\"PRS\", \"OTR\", \"PRS\", \"REP\"]\n",
    "\n",
    "bert.train_with_sentences(sentences, labels, save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('PRS', 0.2611815929412842)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demo single sentence labeling, after training\n",
    "# Might not be accurate, but as we train with larger dataset, it will get better\n",
    "bert.label_sentence(\"I don't know\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this Jupyter notebook, we have successfully deployed a BERT-based AI-assisted labeling system for educational transcripts with dynamic batching and in-loop learning. This system significantly streamlines the process of classifying classroom dialogue into categories like opportunities to respond, praise, reprimands, and neutral comments. By integrating the advanced NLP capabilities of BERT, the system offers contextually aware initial predictions, which are refined through an interactive user interface. This collaborative approach between AI and human input not only improves the accuracy of the labeling over time but also enhances the efficiency of the process.\n",
    "\n",
    "The notebook's design emphasizes user-friendly interaction, adaptability, and scalability, making it a valuable tool for educational data analysis. With the ability to export the labeled data for further use and the system's continuous learning from user feedback, this project demonstrates the potential of AI in transforming educational data processing, paving the way for more insightful and data-driven educational practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
